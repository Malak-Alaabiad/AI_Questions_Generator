{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdm8j5yp8aqj",
        "outputId": "be9c0d74-a259-4d4f-9b4c-c71c443735b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract, PyPDF2, pdf2image\n",
            "Successfully installed PyPDF2-3.0.1 pdf2image-1.17.0 pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 pdf2image pytesseract nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y poppler-utils tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELT3VN7N8lQU",
        "outputId": "cf61c5b8-b52c-4d07-a376-66cec7d3b288"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,696 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Fetched 22.2 MB in 4s (6,167 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.7 [186 kB]\n",
            "Fetched 186 kB in 1s (248 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.7_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiXVdq-X8nsc",
        "outputId": "ee586d3a-771a-449e-c400-a5824ccedf0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Any\n",
        "import PyPDF2\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import files\n",
        "import io\n",
        "from ipywidgets import FileUpload\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "QA7tJEPl8qMO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QGenerator:\n",
        "    def __init__(self):\n",
        "        self.question_templates = {\n",
        "            'factual': [\n",
        "                \"What is {}?\",\n",
        "                \"Who is {}?\",\n",
        "                \"When did {} occur?\",\n",
        "                \"Where is {} located?\",\n",
        "                \"How does {} work?\",\n",
        "                \"What are the components of {}?\",\n",
        "                \"What is the definition of {}?\",\n",
        "                \"What are the characteristics of {}?\",\n",
        "            ],\n",
        "            'conceptual': [\n",
        "                \"Why is {} important?\",\n",
        "                \"How does {} relate to {}?\",\n",
        "                \"What is the significance of {}?\",\n",
        "                \"What are the implications of {}?\",\n",
        "                \"How would you explain {} to someone new to the subject?\",\n",
        "                \"What is the purpose of {}?\",\n",
        "                \"How does {} affect {}?\",\n",
        "            ],\n",
        "            'analytical': [\n",
        "                \"Compare and contrast {} and {}.\",\n",
        "                \"What would happen if {} changed?\",\n",
        "                \"What evidence supports {}?\",\n",
        "                \"How would you analyze {}?\",\n",
        "                \"What conclusions can be drawn from {}?\",\n",
        "                \"What are the strengths and weaknesses of {}?\",\n",
        "                \"How would you evaluate the effectiveness of {}?\",\n",
        "            ],\n",
        "            'application': [\n",
        "                \"How can {} be applied to solve {}?\",\n",
        "                \"What is an example of {} in the real world?\",\n",
        "                \"How would you implement {} in practice?\",\n",
        "                \"What would be a use case for {}?\",\n",
        "                \"How could {} be used to improve {}?\",\n",
        "            ],\n",
        "            'mcq': [\n",
        "                \"Which of the following best describes {}?\",\n",
        "                \"What is the primary function of {}?\",\n",
        "                \"Which statement about {} is correct?\",\n",
        "                \"What is the relationship between {} and {}?\",\n",
        "            ],\n",
        "            'true_false': [\n",
        "                \"{} is a key component of {}.\",\n",
        "                \"{} directly affects {}.\",\n",
        "                \"{} is defined as {}.\",\n",
        "                \"{} can be characterized by {}.\",\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        self.important_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "\n",
        "        self.cover_page_patterns = [\n",
        "            r'cover\\s*page',\n",
        "            r'title\\s*page',\n",
        "            r'copyright',\n",
        "            r'all\\s*rights\\s*reserved',\n",
        "            r'table\\s*of\\s*contents',\n",
        "            r'index',\n",
        "            r'glossary',\n",
        "            r'references',\n",
        "            r'bibliography',\n",
        "            r'appendix',\n",
        "            r'acknowledgements',\n",
        "            r'preface',\n",
        "            r'foreword',\n",
        "        ]\n",
        "\n",
        "        self.noise_patterns = [\n",
        "            r'^\\d+$',\n",
        "            r'^\\s*page\\s*\\d+\\s*$',\n",
        "            r'^\\s*\\d+\\s*of\\s*\\d+\\s*$',\n",
        "            r'^\\s*chapter\\s*\\d+\\s*$',\n",
        "            r'^\\s*section\\s*\\d+\\.*\\d*\\s*$',\n",
        "            r'^\\s*www\\..+\\..+\\s*$',\n",
        "            r'^\\s*https?://.+\\s*$',\n",
        "            r'^\\s*[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\s*$',\n",
        "            r'^\\s*[a-zA-Z0-9\\s]+\\s*\\|\\s*[a-zA-Z0-9\\s]+\\s*$',\n",
        "            r'^\\s*[a-zA-Z0-9\\s]+\\s*-\\s*\\d+\\s*$',\n",
        "        ]\n",
        "\n",
        "        self.unimportant_content_patterns = [\n",
        "            r'^\\s*note\\s*:',\n",
        "            r'^\\s*disclaimer\\s*:',\n",
        "            r'^\\s*caution\\s*:',\n",
        "            r'^\\s*warning\\s*:',\n",
        "            r'^\\s*example\\s*\\d*\\s*:',\n",
        "            r'^\\s*figure\\s*\\d+\\s*:',\n",
        "            r'^\\s*table\\s*\\d+\\s*:',\n",
        "        ]\n",
        "\n",
        "    def _is_cover_page(self, text):\n",
        "        for pattern in self.cover_page_patterns:\n",
        "            if re.search(pattern, text.lower()):\n",
        "                return True\n",
        "\n",
        "        if len(text.strip()) < 200:\n",
        "            return True\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) < 3:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_noise(self, text):\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            if not line.strip():\n",
        "                continue\n",
        "\n",
        "            if any(re.search(pattern, line.strip()) for pattern in self.noise_patterns):\n",
        "                continue\n",
        "\n",
        "            if len(line.strip()) < 5:\n",
        "                continue\n",
        "\n",
        "            if i < 3 or i > len(lines) - 4:\n",
        "                if len(line.strip()) < 50:\n",
        "                    continue\n",
        "\n",
        "            if any(re.search(pattern, line.strip()) for pattern in self.unimportant_content_patterns):\n",
        "                continue\n",
        "\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(cleaned_lines)\n",
        "\n",
        "    def _calculate_content_relevance(self, text):\n",
        "        sentences = sent_tokenize(text)\n",
        "        sentence_count = len(sentences)\n",
        "\n",
        "        words = word_tokenize(text.lower())\n",
        "        unique_words = set(words)\n",
        "        unique_word_count = len(unique_words)\n",
        "\n",
        "        if len(words) > 0:\n",
        "            info_density = unique_word_count / len(words)\n",
        "        else:\n",
        "            info_density = 0\n",
        "\n",
        "        relevance_score = sentence_count * info_density\n",
        "\n",
        "        return relevance_score\n",
        "\n",
        "    def _extract_text_from_pdf(self, pdf_path):\n",
        "        page_texts = []\n",
        "        try:\n",
        "            with open(pdf_path, \"rb\") as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                start_page = 0\n",
        "                if len(reader.pages) > 1:\n",
        "                    first_page_text = reader.pages[0].extract_text() or \"\"\n",
        "                    if self._is_cover_page(first_page_text):\n",
        "                        start_page = 1\n",
        "                        print(\"Skipping cover page...\")\n",
        "\n",
        "                for page_num, page in enumerate(reader.pages[start_page:], start_page + 1):\n",
        "                    page_text = page.extract_text() or \"\"\n",
        "\n",
        "                    if not page_text.strip() or self._is_cover_page(page_text):\n",
        "                        continue\n",
        "\n",
        "                    cleaned_text = self._clean_noise(page_text)\n",
        "\n",
        "                    relevance_score = self._calculate_content_relevance(cleaned_text)\n",
        "\n",
        "                    if relevance_score > 5:\n",
        "                        page_texts.append({\n",
        "                            'text': cleaned_text,\n",
        "                            'page': page_num,\n",
        "                            'source': 'direct',\n",
        "                            'relevance': relevance_score\n",
        "                        })\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text directly: {str(e)}\")\n",
        "\n",
        "        return page_texts\n",
        "\n",
        "    def _extract_text_from_pdf_images(self, pdf_path):\n",
        "        page_texts = []\n",
        "        try:\n",
        "            images = convert_from_path(pdf_path)\n",
        "\n",
        "            start_page = 0\n",
        "            if len(images) > 1:\n",
        "                first_page_text = pytesseract.image_to_string(images[0])\n",
        "                if self._is_cover_page(first_page_text):\n",
        "                    start_page = 1\n",
        "                    print(\"Skipping cover page image...\")\n",
        "\n",
        "            for i, image in enumerate(images[start_page:], start_page + 1):\n",
        "                page_text = pytesseract.image_to_string(image)\n",
        "\n",
        "                if not page_text.strip() or self._is_cover_page(page_text):\n",
        "                    continue\n",
        "\n",
        "                cleaned_text = self._clean_noise(page_text)\n",
        "\n",
        "                relevance_score = self._calculate_content_relevance(cleaned_text)\n",
        "\n",
        "                if relevance_score > 5:\n",
        "                    page_texts.append({\n",
        "                        'text': cleaned_text,\n",
        "                        'page': i,\n",
        "                        'source': 'image',\n",
        "                        'relevance': relevance_score\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from images: {str(e)}\")\n",
        "\n",
        "        return page_texts\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s.,;:!?()-]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _extract_important_phrases(self, page_texts):\n",
        "        important_phrases = []\n",
        "\n",
        "        for page_data in page_texts:\n",
        "            text = page_data['text']\n",
        "            page_num = page_data['page']\n",
        "\n",
        "            sentences = sent_tokenize(text)\n",
        "            for sentence in sentences:\n",
        "                words = word_tokenize(sentence)\n",
        "                tagged_words = pos_tag(words)\n",
        "\n",
        "                current_phrase = []\n",
        "                for word, tag in tagged_words:\n",
        "                    if tag in self.important_pos_tags and word.lower() not in stopwords.words('english'):\n",
        "                        current_phrase.append(word)\n",
        "                    elif current_phrase:\n",
        "                        if len(current_phrase) >= 1:\n",
        "                            phrase = ' '.join(current_phrase)\n",
        "                            important_phrases.append({\n",
        "                                'phrase': phrase,\n",
        "                                'page': page_num,\n",
        "                                'context': sentence\n",
        "                            })\n",
        "                        current_phrase = []\n",
        "\n",
        "                if current_phrase and len(current_phrase) >= 1:\n",
        "                    phrase = ' '.join(current_phrase)\n",
        "                    important_phrases.append({\n",
        "                        'phrase': phrase,\n",
        "                        'page': page_num,\n",
        "                        'context': sentence\n",
        "                    })\n",
        "\n",
        "        unique_phrases = {}\n",
        "        for item in important_phrases:\n",
        "            phrase = item['phrase']\n",
        "            if phrase not in unique_phrases or len(phrase) > len(unique_phrases[phrase]['phrase']):\n",
        "                unique_phrases[phrase] = item\n",
        "\n",
        "        result = list(unique_phrases.values())\n",
        "        result.sort(key=lambda x: len(x['phrase']), reverse=True)\n",
        "\n",
        "        return result[:50]\n",
        "\n",
        "    def _extract_key_sentences(self, page_texts):\n",
        "        key_sentences = []\n",
        "\n",
        "        importance_indicators = [\n",
        "            'important', 'significant', 'key', 'main', 'primary', 'essential', 'critical',\n",
        "            'fundamental', 'crucial', 'vital', 'major', 'central', 'core', 'notable',\n",
        "            'define', 'definition', 'means', 'refer', 'example', 'instance', 'illustrate',\n",
        "            'demonstrate', 'show', 'prove', 'evidence', 'research', 'study', 'analysis',\n",
        "            'conclude', 'summary', 'therefore', 'thus', 'hence', 'consequently'\n",
        "        ]\n",
        "\n",
        "        for page_data in page_texts:\n",
        "            text = page_data['text']\n",
        "            page_num = page_data['page']\n",
        "\n",
        "            sentences = sent_tokenize(text)\n",
        "            for sentence in sentences:\n",
        "                if any(indicator in sentence.lower() for indicator in importance_indicators):\n",
        "                    key_sentences.append({\n",
        "                        'sentence': sentence,\n",
        "                        'page': page_num\n",
        "                    })\n",
        "                elif re.search(r'\\b[A-Z][a-z]+ (is|are|refers to|means|can be defined as)\\b', sentence):\n",
        "                    key_sentences.append({\n",
        "                        'sentence': sentence,\n",
        "                        'page': page_num\n",
        "                    })\n",
        "\n",
        "        return key_sentences\n",
        "\n",
        "    def _find_answer_for_phrase(self, phrase, page_texts, context=None):\n",
        "        for page_data in page_texts:\n",
        "            text = page_data['text']\n",
        "            page_num = page_data['page']\n",
        "\n",
        "            definition_patterns = [\n",
        "                f\"{phrase} is \",\n",
        "                f\"{phrase} are \",\n",
        "                f\"{phrase} refers to \",\n",
        "                f\"{phrase} means \",\n",
        "                f\"{phrase} can be defined as \",\n",
        "                f\"define {phrase} as \",\n",
        "                f\"{phrase} is defined as \"\n",
        "            ]\n",
        "\n",
        "            sentences = sent_tokenize(text)\n",
        "            for sentence in sentences:\n",
        "                if any(pattern.lower() in sentence.lower() for pattern in definition_patterns):\n",
        "                    return {\n",
        "                        'answer': sentence,\n",
        "                        'page': page_num\n",
        "                    }\n",
        "\n",
        "        if context:\n",
        "            for page_data in page_texts:\n",
        "                if context in page_data['text']:\n",
        "                    return {\n",
        "                        'answer': context,\n",
        "                        'page': page_data['page']\n",
        "                    }\n",
        "\n",
        "        relevant_sentences = []\n",
        "        for page_data in page_texts:\n",
        "            text = page_data['text']\n",
        "            page_num = page_data['page']\n",
        "\n",
        "            sentences = sent_tokenize(text)\n",
        "            for sentence in sentences:\n",
        "                if phrase.lower() in sentence.lower():\n",
        "                    relevant_sentences.append({\n",
        "                        'sentence': sentence,\n",
        "                        'page': page_num,\n",
        "                        'relevance': len(sentence)\n",
        "                    })\n",
        "\n",
        "        relevant_sentences.sort(key=lambda x: x['relevance'])\n",
        "\n",
        "        if relevant_sentences:\n",
        "            best_match = relevant_sentences[0]\n",
        "            return {\n",
        "                'answer': best_match['sentence'],\n",
        "                'page': best_match['page']\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'answer': f\"Information about {phrase} can be found in the document.\",\n",
        "            'page': 'unknown'\n",
        "        }\n",
        "\n",
        "    def _generate_factual_questions_with_answers(self, phrases, page_texts, num):\n",
        "        questions = []\n",
        "        templates = self.question_templates['factual']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for phrase_data in phrases_copy[:num]:\n",
        "            phrase = phrase_data['phrase']\n",
        "            context = phrase_data.get('context')\n",
        "            template = random.choice(templates)\n",
        "            question = template.format(phrase)\n",
        "\n",
        "            answer_data = self._find_answer_for_phrase(phrase, page_texts, context)\n",
        "\n",
        "            questions.append({\n",
        "                'question': question,\n",
        "                'answer': answer_data['answer'],\n",
        "                'page': answer_data['page'],\n",
        "                'type': 'factual'\n",
        "            })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_mcq_questions(self, phrases, page_texts, num):\n",
        "        questions = []\n",
        "        templates = self.question_templates['mcq']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for i in range(min(num, len(phrases_copy))):\n",
        "            phrase_data = phrases_copy[i]\n",
        "            phrase = phrase_data['phrase']\n",
        "            context = phrase_data.get('context')\n",
        "            template = random.choice(templates)\n",
        "\n",
        "            if \"{}\" in template and \"{}\" in template[template.index(\"{}\") + 2:]:\n",
        "                if i + 1 < len(phrases_copy):\n",
        "                    phrase2 = phrases_copy[i+1]['phrase']\n",
        "                    question = template.format(phrase, phrase2)\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                question = template.format(phrase)\n",
        "\n",
        "            answer_data = self._find_answer_for_phrase(phrase, page_texts, context)\n",
        "            correct_answer = answer_data['answer']\n",
        "\n",
        "            options = [correct_answer]\n",
        "\n",
        "            for _ in range(3):\n",
        "                if len(phrases_copy) > i + 2:\n",
        "                    wrong_phrase = phrases_copy[i + 2]['phrase']\n",
        "                    wrong_answer_data = self._find_answer_for_phrase(wrong_phrase, page_texts)\n",
        "                    if wrong_answer_data['answer'] != correct_answer:\n",
        "                        options.append(wrong_answer_data['answer'])\n",
        "                    i += 1\n",
        "\n",
        "            while len(options) < 4:\n",
        "                options.append(f\"None of the above statements about {phrase} are correct.\")\n",
        "\n",
        "            random.shuffle(options)\n",
        "\n",
        "            questions.append({\n",
        "                'question': question,\n",
        "                'options': options,\n",
        "                'answer': correct_answer,\n",
        "                'page': answer_data['page'],\n",
        "                'type': 'mcq'\n",
        "            })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_true_false_questions(self, phrases, page_texts, num):\n",
        "        questions = []\n",
        "        templates = self.question_templates['true_false']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for i in range(min(num, len(phrases_copy))):\n",
        "            phrase_data = phrases_copy[i]\n",
        "            phrase = phrase_data['phrase']\n",
        "            context = phrase_data.get('context')\n",
        "            template = random.choice(templates)\n",
        "\n",
        "            if \"{}\" in template and \"{}\" in template[template.index(\"{}\") + 2:]:\n",
        "                if i + 1 < len(phrases_copy):\n",
        "                    phrase2 = phrases_copy[i+1]['phrase']\n",
        "                    statement = template.format(phrase, phrase2)\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            answer_data = self._find_answer_for_phrase(phrase, page_texts, context)\n",
        "\n",
        "            is_true = random.choice([True, False])\n",
        "\n",
        "            if not is_true:\n",
        "                if i + 2 < len(phrases_copy):\n",
        "                    wrong_phrase = phrases_copy[i+2]['phrase']\n",
        "                    statement = template.format(phrase, wrong_phrase)\n",
        "\n",
        "            questions.append({\n",
        "                'question': f\"True or False: {statement}\",\n",
        "                'answer': \"True\" if is_true else \"False\",\n",
        "                'page': answer_data['page'],\n",
        "                'type': 'true_false'\n",
        "            })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_conceptual_questions_with_answers(self, phrases, page_texts, num):\n",
        "        questions = []\n",
        "        templates = self.question_templates['conceptual']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for i, template in enumerate(random.sample(templates, min(num, len(templates)))):\n",
        "            if \"{}\" in template and \"{}\" in template[template.index(\"{}\") + 2:]:\n",
        "                if len(phrases_copy) >= 2:\n",
        "                    phrase_data1 = phrases_copy.pop(0)\n",
        "                    phrase_data2 = phrases_copy.pop(0)\n",
        "                    phrase1 = phrase_data1['phrase']\n",
        "                    phrase2 = phrase_data2['phrase']\n",
        "                    question = template.format(phrase1, phrase2)\n",
        "\n",
        "                    both_phrases = f\"{phrase1} and {phrase2}\"\n",
        "                    answer_data = self._find_answer_for_phrase(both_phrases, page_texts)\n",
        "\n",
        "                    if answer_data['page'] == 'unknown':\n",
        "                        answer_data = self._find_answer_for_phrase(phrase1, page_texts, phrase_data1.get('context'))\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': answer_data['answer'],\n",
        "                        'page': answer_data['page'],\n",
        "                        'type': 'conceptual'\n",
        "                    })\n",
        "            else:\n",
        "                if phrases_copy:\n",
        "                    phrase_data = phrases_copy.pop(0)\n",
        "                    phrase = phrase_data['phrase']\n",
        "                    question = template.format(phrase)\n",
        "\n",
        "                    answer_data = self._find_answer_for_phrase(phrase, page_texts, phrase_data.get('context'))\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': answer_data['answer'],\n",
        "                        'page': answer_data['page'],\n",
        "                        'type': 'conceptual'\n",
        "                    })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_analytical_questions_with_answers(self, phrases, key_sentences, page_texts, num):\n",
        "        questions = []\n",
        "        templates = self.question_templates['analytical']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for i, template in enumerate(random.sample(templates, min(num, len(templates)))):\n",
        "            if \"{}\" in template and \"{}\" in template[template.index(\"{}\") + 2:]:\n",
        "                if len(phrases_copy) >= 2:\n",
        "                    phrase_data1 = phrases_copy.pop(0)\n",
        "                    phrase_data2 = phrases_copy.pop(0)\n",
        "                    phrase1 = phrase_data1['phrase']\n",
        "                    phrase2 = phrase_data2['phrase']\n",
        "                    question = template.format(phrase1, phrase2)\n",
        "\n",
        "                    answer1 = self._find_answer_for_phrase(phrase1, page_texts, phrase_data1.get('context'))\n",
        "                    answer2 = self._find_answer_for_phrase(phrase2, page_texts, phrase_data2.get('context'))\n",
        "\n",
        "                    combined_answer = f\"Regarding {phrase1}: {answer1['answer']} Regarding {phrase2}: {answer2['answer']}\"\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': combined_answer,\n",
        "                        'page': f\"{answer1['page']} and {answer2['page']}\",\n",
        "                        'type': 'analytical'\n",
        "                    })\n",
        "            else:\n",
        "                if phrases_copy:\n",
        "                    phrase_data = phrases_copy.pop(0)\n",
        "                    phrase = phrase_data['phrase']\n",
        "                    question = template.format(phrase)\n",
        "\n",
        "                    answer_data = self._find_answer_for_phrase(phrase, page_texts, phrase_data.get('context'))\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': answer_data['answer'],\n",
        "                        'page': answer_data['page'],\n",
        "                        'type': 'analytical'\n",
        "                    })\n",
        "\n",
        "        if key_sentences and len(questions) < num:\n",
        "            for sentence_data in random.sample(key_sentences, min(num - len(questions), len(key_sentences))):\n",
        "                sentence = sentence_data['sentence']\n",
        "                page = sentence_data['page']\n",
        "\n",
        "                question = f\"Analyze the following statement: '{sentence}'\"\n",
        "\n",
        "                context_answer = self._find_context_for_sentence(sentence, page_texts, page)\n",
        "\n",
        "                questions.append({\n",
        "                    'question': question,\n",
        "                    'answer': context_answer['answer'],\n",
        "                    'page': context_answer['page'],\n",
        "                    'type': 'analytical'\n",
        "                })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _find_context_for_sentence(self, sentence, page_texts, target_page):\n",
        "        for page_data in page_texts:\n",
        "            if page_data['page'] == target_page:\n",
        "                text = page_data['text']\n",
        "\n",
        "                sentences = sent_tokenize(text)\n",
        "                for i, s in enumerate(sentences):\n",
        "                    if sentence in s:\n",
        "                        context = []\n",
        "                        if i > 0:\n",
        "                            context.append(sentences[i-1])\n",
        "                        context.append(s)\n",
        "                        if i < len(sentences) - 1:\n",
        "                            context.append(sentences[i+1])\n",
        "\n",
        "                        return {\n",
        "                            'answer': ' '.join(context),\n",
        "                            'page': target_page\n",
        "                        }\n",
        "\n",
        "        return {\n",
        "            'answer': sentence,\n",
        "            'page': target_page\n",
        "        }\n",
        "\n",
        "    def _generate_application_questions_with_answers(self, phrases, page_texts, num):\n",
        "        questions = []\n",
        "        templates = self.question_templates['application']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for i, template in enumerate(random.sample(templates, min(num, len(templates)))):\n",
        "            if \"{}\" in template and \"{}\" in template[template.index(\"{}\") + 2:]:\n",
        "                if len(phrases_copy) >= 2:\n",
        "                    phrase_data1 = phrases_copy.pop(0)\n",
        "                    phrase_data2 = phrases_copy.pop(0)\n",
        "                    phrase1 = phrase_data1['phrase']\n",
        "                    phrase2 = phrase_data2['phrase']\n",
        "                    question = template.format(phrase1, phrase2)\n",
        "\n",
        "                    answer1 = self._find_answer_for_phrase(phrase1, page_texts, phrase_data1.get('context'))\n",
        "                    answer2 = self._find_answer_for_phrase(phrase2, page_texts, phrase_data2.get('context'))\n",
        "\n",
        "                    combined_answer = f\"Based on the document: Regarding {phrase1}: {answer1['answer']} Regarding {phrase2}: {answer2['answer']}\"\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': combined_answer,\n",
        "                        'page': f\"{answer1['page']} and {answer2['page']}\",\n",
        "                        'type': 'application'\n",
        "                    })\n",
        "            else:\n",
        "                if phrases_copy:\n",
        "                    phrase_data = phrases_copy.pop(0)\n",
        "                    phrase = phrase_data['phrase']\n",
        "                    question = template.format(phrase)\n",
        "\n",
        "                    answer_data = self._find_answer_for_phrase(phrase, page_texts, phrase_data.get('context'))\n",
        "\n",
        "                    answer = f\"Based on the document: {answer_data['answer']}\"\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': answer,\n",
        "                        'page': answer_data['page'],\n",
        "                        'type': 'application'\n",
        "                    })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def generate_questions_with_answers_from_pdf(self, pdf_path, num_questions=10):\n",
        "        print(\"Extracting text from PDF and filtering out cover pages, headers, footers, and unimportant content...\")\n",
        "\n",
        "        direct_page_texts = self._extract_text_from_pdf(pdf_path)\n",
        "        image_page_texts = self._extract_text_from_pdf_images(pdf_path)\n",
        "\n",
        "        all_page_texts = direct_page_texts + image_page_texts\n",
        "        all_page_texts.sort(key=lambda x: x.get('relevance', 0), reverse=True)\n",
        "\n",
        "        most_relevant_pages = all_page_texts[:min(len(all_page_texts), 20)]\n",
        "\n",
        "        for page_data in most_relevant_pages:\n",
        "            page_data['text'] = self._clean_text(page_data['text'])\n",
        "\n",
        "        total_text = ''.join(page_data['text'] for page_data in most_relevant_pages)\n",
        "        if not total_text or len(total_text) < 50:\n",
        "            return [{\n",
        "                'question': \"The PDF doesn't contain enough extractable text to generate questions.\",\n",
        "                'answer': \"Please try a different PDF with more text content.\",\n",
        "                'page': \"N/A\",\n",
        "                'type': 'error'\n",
        "            }]\n",
        "\n",
        "        print(f\"Successfully extracted content from {len(most_relevant_pages)} relevant pages.\")\n",
        "\n",
        "        important_phrases = self._extract_important_phrases(most_relevant_pages)\n",
        "        key_sentences = self._extract_key_sentences(most_relevant_pages)\n",
        "\n",
        "        print(f\"Identified {len(important_phrases)} important phrases and {len(key_sentences)} key sentences.\")\n",
        "\n",
        "        num_factual = max(1, int(num_questions * 0.25))\n",
        "        num_conceptual = max(1, int(num_questions * 0.15))\n",
        "        num_analytical = max(1, int(num_questions * 0.15))\n",
        "        num_application = max(1, int(num_questions * 0.15))\n",
        "        num_mcq = max(1, int(num_questions * 0.15))\n",
        "        num_true_false = max(1, num_questions - num_factual - num_conceptual - num_analytical - num_application - num_mcq)\n",
        "\n",
        "        print(\"Generating diverse question types and finding answers...\")\n",
        "\n",
        "        factual_questions = self._generate_factual_questions_with_answers(important_phrases, most_relevant_pages, num_factual)\n",
        "        conceptual_questions = self._generate_conceptual_questions_with_answers(important_phrases, most_relevant_pages, num_conceptual)\n",
        "        analytical_questions = self._generate_analytical_questions_with_answers(important_phrases, key_sentences, most_relevant_pages, num_analytical)\n",
        "        application_questions = self._generate_application_questions_with_answers(important_phrases, most_relevant_pages, num_application)\n",
        "        mcq_questions = self._generate_mcq_questions(important_phrases, most_relevant_pages, num_mcq)\n",
        "        true_false_questions = self._generate_true_false_questions(important_phrases, most_relevant_pages, num_true_false)\n",
        "\n",
        "        all_questions = factual_questions + conceptual_questions + analytical_questions + application_questions + mcq_questions + true_false_questions\n",
        "\n",
        "        if len(all_questions) < num_questions and important_phrases:\n",
        "            additional_needed = num_questions - len(all_questions)\n",
        "            additional_questions = self._generate_factual_questions_with_answers(important_phrases, most_relevant_pages, additional_needed)\n",
        "            all_questions.extend(additional_questions)\n",
        "\n",
        "        random.shuffle(all_questions)\n",
        "\n",
        "        print(f\"Successfully generated {len(all_questions)} questions with answers.\")\n",
        "\n",
        "        return all_questions[:num_questions]\n",
        "\n",
        "def generate_questions_with_answers_from_uploaded_pdf(num_questions=10):\n",
        "    print(\"Please upload a PDF file...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file was uploaded.\")\n",
        "        return\n",
        "\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "\n",
        "    print(f\"\\nProcessing {file_name}...\")\n",
        "    print(\"This may take a few minutes depending on the PDF size and complexity.\")\n",
        "\n",
        "    generator = QGenerator()\n",
        "\n",
        "    questions_with_answers = generator.generate_questions_with_answers_from_pdf(file_name, num_questions)\n",
        "\n",
        "    print(f\"\\nGenerated {len(questions_with_answers)} questions with answers from {file_name}:\")\n",
        "    for i, qa in enumerate(questions_with_answers, 1):\n",
        "        print(f\"\\n{i}. {qa['question']}\")\n",
        "        if qa['type'] == 'mcq':\n",
        "            print(f\"   Options:\")\n",
        "            for j, option in enumerate(qa['options'], 1):\n",
        "                print(f\"     {j}. {option}\")\n",
        "            print(f\"   Correct Answer: {qa['answer']}\")\n",
        "        else:\n",
        "            print(f\"   Answer: {qa['answer']}\")\n",
        "        print(f\"   Page: {qa['page']}\")\n",
        "        print(f\"   Type: {qa['type']}\")\n",
        "\n",
        "    return questions_with_answers"
      ],
      "metadata": {
        "id": "AGm8N7aE8upD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_questions = 50\n",
        "\n",
        "print(\"Setting up to generate questions from your PDF...\")\n",
        "questions_with_answers = generate_questions_with_answers_from_uploaded_pdf(num_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T9_5XFdh8xqB",
        "outputId": "c9674055-c2c2-4c8d-f9b7-2a8fcfb797f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up to generate questions from your PDF...\n",
            "Please upload a PDF file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-422771e8-f6fd-421b-9383-a538738bab5e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-422771e8-f6fd-421b-9383-a538738bab5e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Research .pdf to Research .pdf\n",
            "\n",
            "Processing Research .pdf...\n",
            "This may take a few minutes depending on the PDF size and complexity.\n",
            "Extracting text from PDF and filtering out cover pages, headers, footers, and unimportant content...\n",
            "Skipping cover page...\n",
            "Skipping cover page image...\n",
            "Successfully extracted content from 4 relevant pages.\n",
            "Identified 50 important phrases and 19 key sentences.\n",
            "Generating diverse question types and finding answers...\n",
            "Successfully generated 50 questions with answers.\n",
            "\n",
            "Generated 50 questions with answers from Research .pdf:\n",
            "\n",
            "1. When did linear transformation occur?\n",
            "   Answer: Understanding the Concept Lets break it down with a geometric interpretation:  Amatrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "2. True or False: dominant eigenvector can be characterized by corresponding eigenvalues tell.\n",
            "   Answer: False\n",
            "   Page: 3\n",
            "   Type: true_false\n",
            "\n",
            "3. What is an example of Computer Science Eigenvalues in the real world?\n",
            "   Answer: Based on the document: Applications in Computer Science Eigenvalues and eigenvectors have a wide range of applications in computer science.\n",
            "   Page: 2\n",
            "   Type: application\n",
            "\n",
            "4. What are the components of Concept Lets break?\n",
            "   Answer: Understanding the Concept Lets break it down with a geometric interpretation:  Amatrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "5. True or False: magnitude changes is defined as image processing.\n",
            "   Answer: True\n",
            "   Page: 2\n",
            "   Type: true_false\n",
            "\n",
            "6. Which statement about modified link matrix is correct?\n",
            "   Options:\n",
            "     1. In simple terms, an eigenvector of a matrix is a non-zero vector that only changes by a scalar factor when that matrix is applied to it.\n",
            "     2. How: PCA uses the eigenvectors of the datas covariance matrix to identify the directions (principal components) where the data varies the most.\n",
            "     3. It calculates the dominant eigenvector of a modified link matrix to determine the importance of each webpage.\n",
            "     4. Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Correct Answer: It calculates the dominant eigenvector of a modified link matrix to determine the importance of each webpage.\n",
            "   Page: 3\n",
            "   Type: mcq\n",
            "\n",
            "7. Compare and contrast Improves search result accuracy and image processing.\n",
            "   Answer: Regarding Improves search result accuracy: Benefit: Improves search result accuracy and relevance. Regarding image processing: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 3 and 2\n",
            "   Type: analytical\n",
            "\n",
            "8. What is the relationship between matrix represents and eigenface recognition?\n",
            "   Options:\n",
            "     1. How: Googles PageRank treats the web as a graph and uses eigenvector centrality to rank pages.\n",
            "     2. Understanding the Concept Lets break it down with a geometric interpretation:  A matrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "     3. Understanding the Concept Lets break it down with a geometric interpretation:  Amatrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "     4. Use Case: Analyzing social networks, finding influential nodes.\n",
            "   Correct Answer: Understanding the Concept Lets break it down with a geometric interpretation:  A matrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "   Page: 2\n",
            "   Type: mcq\n",
            "\n",
            "9. How does Simplifies data affect dominant eigenvector?\n",
            "   Answer: Benefit: Simplifies data, reduces noise, and speeds up machine learning models.\n",
            "   Page: 3\n",
            "   Type: conceptual\n",
            "\n",
            "10. What would happen if uses eigenvector centrality changed?\n",
            "   Answer: How: Googles PageRank treats the web as a graph and uses eigenvector centrality to rank pages.\n",
            "   Page: 3\n",
            "   Type: analytical\n",
            "\n",
            "11. Who is Googles PageRank Algorithm Use Case?\n",
            "   Answer: Googles PageRank Algorithm Use Case: Ranking web pages in search engines.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "12. Where is vectors represent key facial features located?\n",
            "   Answer: These vectors represent key facial features.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "13. What are the characteristics of machine learning?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "14. True or False: corresponding eigenvalues tell can be characterized by Understanding eigenvalues.\n",
            "   Answer: True\n",
            "   Page: 3\n",
            "   Type: true_false\n",
            "\n",
            "15. Who is computer graphics?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "16. Who is computer science?\n",
            "   Answer: Applications in Computer Science Eigenvalues and eigenvectors have a wide range of applications in computer science.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "17. True or False: Analyzing social networks directly affects computer science.\n",
            "   Answer: True\n",
            "   Page: 3\n",
            "   Type: true_false\n",
            "\n",
            "18. What is the definition of linear transformations?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "19. True or False: non -zero vector is a key component of Analyzing social networks.\n",
            "   Answer: True\n",
            "   Page: 2\n",
            "   Type: true_false\n",
            "\n",
            "20. True or False: linear algebra is a key component of Understanding eigenvalues.\n",
            "   Answer: False\n",
            "   Page: 2\n",
            "   Type: true_false\n",
            "\n",
            "21. Which of the following best describes non-zero vector?\n",
            "   Options:\n",
            "     1. In simple terms, an eigenvector of a matrix is a non-zero vector that only changes by a scalar factor when that matrix is applied to it.\n",
            "     2. Use Case: Analyzing social networks, finding influential nodes.\n",
            "     3. Understanding the Concept Lets break it down with a geometric interpretation:  Amatrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "     4. How: In eigenface recognition, each face image is projected onto the eigenvectors (eigenfaces) of the training set.\n",
            "   Correct Answer: In simple terms, an eigenvector of a matrix is a non-zero vector that only changes by a scalar factor when that matrix is applied to it.\n",
            "   Page: 2\n",
            "   Type: mcq\n",
            "\n",
            "22. How would you analyze fundamental concepts?\n",
            "   Answer: Eigenvalues and eigenvectors are fundamental concepts in linear algebra, which play a significant role in various fields of science and engineering.\n",
            "   Page: 2\n",
            "   Type: analytical\n",
            "\n",
            "23. How would you evaluate the effectiveness of Computer Vision?\n",
            "   Answer: Computer Vision and Image Compression Use Case: Image compression using PCA or Eigenfaces in facial recognition.\n",
            "   Page: 3\n",
            "   Type: analytical\n",
            "\n",
            "24. Where is Network Analysis Use Case located?\n",
            "   Answer: Graph Theory and Network Analysis Use Case: Analyzing social networks, finding influential nodes.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "25. What is the significance of Understandin g eigenvalues?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and λ is the eigenvalue if: Understandin g eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: conceptual\n",
            "\n",
            "26. True or False: datas covariance matrix is a key component of magnitude changes.\n",
            "   Answer: True\n",
            "   Page: 3\n",
            "   Type: true_false\n",
            "\n",
            "27. What evidence supports eigenface recognition?\n",
            "   Answer: How: In eigenface recognition, each face image is projected onto the eigenvectors (eigenfaces) of the training set.\n",
            "   Page: 3\n",
            "   Type: analytical\n",
            "\n",
            "28. What are the components of eigenvectors allows?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "29. Which statement about datas covariance matrix is correct?\n",
            "   Options:\n",
            "     1. How: Googles PageRank treats the web as a graph and uses eigenvector centrality to rank pages.\n",
            "     2. Use Case: Analyzing social networks, finding influential nodes.\n",
            "     3. How: PCA uses the eigenvectors of the datas covariance matrix to identify the directions (principal components) where the data varies the most.\n",
            "     4. How: In eigenface recognition, each face image is projected onto the eigenvectors (eigenfaces) of the training set.\n",
            "   Correct Answer: How: PCA uses the eigenvectors of the datas covariance matrix to identify the directions (principal components) where the data varies the most.\n",
            "   Page: 3\n",
            "   Type: mcq\n",
            "\n",
            "30. What is the primary function of Understanding eigenvalues?\n",
            "   Options:\n",
            "     1. Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "     2. Understanding the Concept Lets break it down with a geometric interpretation:  Amatrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "     3. How: PCA uses the eigenvectors of the datas covariance matrix to identify the directions (principal components) where the data varies the most.\n",
            "     4. How: In eigenface recognition, each face image is projected onto the eigenvectors (eigenfaces) of the training set.\n",
            "   Correct Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: mcq\n",
            "\n",
            "31. True or False: Understanding eigenvalues is a key component of datas covariance matrix.\n",
            "   Answer: True\n",
            "   Page: 2\n",
            "   Type: true_false\n",
            "\n",
            "32. True or False: Machine Learning is defined as Analyzing social networks.\n",
            "   Answer: False\n",
            "   Page: 3\n",
            "   Type: true_false\n",
            "\n",
            "33. Why is Ranking web pages important?\n",
            "   Answer: Googles PageRank Algorithm Use Case: Ranking web pages in search engines.\n",
            "   Page: 3\n",
            "   Type: conceptual\n",
            "\n",
            "34. What is the primary function of Efficient storage?\n",
            "   Options:\n",
            "     1. How: PCA uses the eigenvectors of the datas covariance matrix to identify the directions (principal components) where the data varies the most.\n",
            "     2. Benefit: Efficient storage and faster facial recognition.\n",
            "     3. Understanding the Concept Lets break it down with a geometric interpretation:  Amatrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "     4. In simple terms, an eigenvector of a matrix is a non-zero vector that only changes by a scalar factor when that matrix is applied to it.\n",
            "   Correct Answer: Benefit: Efficient storage and faster facial recognition.\n",
            "   Page: 3\n",
            "   Type: mcq\n",
            "\n",
            "35. What are the implications of search engines?\n",
            "   Answer: Googles PageRank Algorithm Use Case: Ranking web pages in search engines.\n",
            "   Page: 3\n",
            "   Type: conceptual\n",
            "\n",
            "36. How could datas covariance matrix be used to improve Ranking web pages?\n",
            "   Answer: Based on the document: Regarding datas covariance matrix: How: PCA uses the eigenvectors of the datas covariance matrix to identify the directions (principal components) where the data varies the most. Regarding Ranking web pages: Googles PageRank Algorithm Use Case: Ranking web pages in search engines.\n",
            "   Page: 3 and 3\n",
            "   Type: application\n",
            "\n",
            "37. How does eigenvectors allows relate to fundamental concepts?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: conceptual\n",
            "\n",
            "38. What is the purpose of linear transformations?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: conceptual\n",
            "\n",
            "39. How would you implement non-zero vector in practice?\n",
            "   Answer: Based on the document: In simple terms, an eigenvector of a matrix is a non-zero vector that only changes by a scalar factor when that matrix is applied to it.\n",
            "   Page: 2\n",
            "   Type: application\n",
            "\n",
            "40. True or False: image processing can be characterized by Machine Learning.\n",
            "   Answer: True\n",
            "   Page: 2\n",
            "   Type: true_false\n",
            "\n",
            "41. Who is image processing?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "42. What would be a use case for uses eigenvector centrality?\n",
            "   Answer: Based on the document: How: Googles PageRank treats the web as a graph and uses eigenvector centrality to rank pages.\n",
            "   Page: 3\n",
            "   Type: application\n",
            "\n",
            "43. How can Googles PageRank treats be applied to solve computer graphics?\n",
            "   Answer: Based on the document: Regarding Googles PageRank treats: How: Googles PageRank treats the web as a graph and uses eigenvector centrality to rank pages. Regarding computer graphics: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 3 and 2\n",
            "   Type: application\n",
            "\n",
            "44. What is the primary function of Machine Learning?\n",
            "   Options:\n",
            "     1. Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "     2. Benefit: Efficient storage and faster facial recognition.\n",
            "     3. Use Case: Dimensionality Reduction, Feature Extraction in Machine Learning.\n",
            "     4. In simple terms, an eigenvector of a matrix is a non-zero vector that only changes by a scalar factor when that matrix is applied to it.\n",
            "   Correct Answer: Use Case: Dimensionality Reduction, Feature Extraction in Machine Learning.\n",
            "   Page: 3\n",
            "   Type: mcq\n",
            "\n",
            "45. What are the strengths and weaknesses of geometric interpretation?\n",
            "   Answer: Understanding the Concept Lets break it down with a geometric interpretation:  Amatrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "   Page: 2\n",
            "   Type: analytical\n",
            "\n",
            "46. How would you explain Improves search result accuracy to someone new to the subject?\n",
            "   Answer: Benefit: Improves search result accuracy and relevance.\n",
            "   Page: 3\n",
            "   Type: conceptual\n",
            "\n",
            "47. What is matrix represents?\n",
            "   Answer: Understanding the Concept Lets break it down with a geometric interpretation:  A matrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "48. What is the definition of Dimensionality Reduction?\n",
            "   Answer: Use Case: Dimensionality Reduction, Feature Extraction in Machine Learning.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "49. What conclusions can be drawn from magnitude changes?\n",
            "   Answer: The directions that remain the same (even if the magnitude changes) under the transformation are represented by eigenvectors.\n",
            "   Page: 2\n",
            "   Type: analytical\n",
            "\n",
            "50. What are the components of Googles PageRank Algorithm Use Case?\n",
            "   Answer: Googles PageRank Algorithm Use Case: Ranking web pages in search engines.\n",
            "   Page: 3\n",
            "   Type: factual\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if questions_with_answers:\n",
        "    with open('questions_and_answers.txt', 'w') as f:\n",
        "        for i, qa in enumerate(questions_with_answers, 1):\n",
        "            f.write(f\"{i}. {qa['question']}\\n\")\n",
        "            if qa['type'] == 'mcq':\n",
        "                f.write(\"   Options:\\n\")\n",
        "                for j, option in enumerate(qa['options'], 1):\n",
        "                    f.write(f\"     {j}. {option}\\n\")\n",
        "                f.write(f\"   Correct Answer: {qa['answer']}\\n\")\n",
        "            else:\n",
        "                f.write(f\"   Answer: {qa['answer']}\\n\")\n",
        "            f.write(f\"   Page: {qa['page']}\\n\")\n",
        "            f.write(f\"   Type: {qa['type']}\\n\\n\")\n",
        "\n",
        "    files.download('questions_and_answers.txt')\n",
        "    print(\"Questions and answers have been saved to 'questions_and_answers.txt' and downloaded.\")"
      ],
      "metadata": {
        "id": "Qbom3zvZ80A9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}