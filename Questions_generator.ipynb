{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz_S_LR8GEFV",
        "outputId": "af486459-7b56-4aa5-8c01-3639dad904c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract, PyPDF2, pdf2image\n",
            "Successfully installed PyPDF2-3.0.1 pdf2image-1.17.0 pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 pdf2image pytesseract nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y poppler-utils tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxTl_T6WGlEY",
        "outputId": "e3532203-f7bf-441a-cdc3-f35446b06c04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,696 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Fetched 22.2 MB in 5s (4,635 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.7 [186 kB]\n",
            "Fetched 186 kB in 0s (1,095 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.7_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y06MhA4RGkd_",
        "outputId": "8af02657-1e22-4bff-da52-44eeeb812647"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Any\n",
        "import PyPDF2\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import files\n",
        "import io"
      ],
      "metadata": {
        "id": "KJjQryRD13GC"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QGenerator:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializing\"\"\"\n",
        "        self.question_templates = {\n",
        "            'factual': [\n",
        "                \"What is {}?\",\n",
        "                \"Who is {}?\",\n",
        "                \"When did {} occur?\",\n",
        "                \"Where is {} located?\",\n",
        "                \"How does {} work?\",\n",
        "                \"What are the components of {}?\",\n",
        "                \"What is the definition of {}?\",\n",
        "                \"What are the characteristics of {}?\",\n",
        "            ],\n",
        "            'conceptual': [\n",
        "                \"Why is {} important?\",\n",
        "                \"How does {} relate to {}?\",\n",
        "                \"What is the significance of {}?\",\n",
        "                \"What are the implications of {}?\",\n",
        "                \"How would you explain {} to someone new to the subject?\",\n",
        "                \"What is the purpose of {}?\",\n",
        "                \"How does {} affect {}?\",\n",
        "            ],\n",
        "            'analytical': [\n",
        "                \"Compare and contrast {} and {}.\",\n",
        "                \"What would happen if {} changed?\",\n",
        "                \"What evidence supports {}?\",\n",
        "                \"How would you analyze {}?\",\n",
        "                \"What conclusions can be drawn from {}?\",\n",
        "                \"What are the strengths and weaknesses of {}?\",\n",
        "                \"How would you evaluate the effectiveness of {}?\",\n",
        "            ],\n",
        "            'application': [\n",
        "                \"How can {} be applied to solve {}?\",\n",
        "                \"What is an example of {} in the real world?\",\n",
        "                \"How would you implement {} in practice?\",\n",
        "                \"What would be a use case for {}?\",\n",
        "                \"How could {} be used to improve {}?\",\n",
        "            ]\n",
        "        }\n",
        "\n",
        "\n",
        "        self.important_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "\n",
        "        self.cover_page_patterns = [\n",
        "            r'cover\\s*page',\n",
        "            r'title\\s*page',\n",
        "            r'copyright',\n",
        "            r'all\\s*rights\\s*reserved',\n",
        "            r'table\\s*of\\s*contents',\n",
        "            r'index',\n",
        "            r'glossary',\n",
        "            r'references',\n",
        "            r'bibliography',\n",
        "            r'appendix',\n",
        "        ]\n",
        "\n",
        "        self.noise_patterns = [\n",
        "            r'^\\d+$',\n",
        "            r'^\\s*page\\s*\\d+\\s*$',\n",
        "            r'^\\s*\\d+\\s*of\\s*\\d+\\s*$',\n",
        "            r'^\\s*chapter\\s*\\d+\\s*$',\n",
        "            r'^\\s*section\\s*\\d+\\.*\\d*\\s*$',\n",
        "            r'^\\s*www\\..+\\..+\\s*$',\n",
        "            r'^\\s*https?://.+\\s*$',\n",
        "            r'^\\s*[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\s*$',\n",
        "        ]\n",
        "\n",
        "    def _is_cover_page(self, text: str) -> bool:\n",
        "        for pattern in self.cover_page_patterns:\n",
        "            if re.search(pattern, text.lower()):\n",
        "                return True\n",
        "\n",
        "        if len(text.strip()) < 200:\n",
        "            return True\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) < 3:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_noise(self, text: str) -> str:\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "\n",
        "            if any(re.search(pattern, line.strip()) for pattern in self.noise_patterns):\n",
        "                continue\n",
        "\n",
        "            if len(line.strip()) < 5:\n",
        "                continue\n",
        "\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(cleaned_lines)\n",
        "\n",
        "    def _calculate_content_relevance(self, text: str) -> float:\n",
        "        sentences = sent_tokenize(text)\n",
        "        sentence_count = len(sentences)\n",
        "\n",
        "        words = word_tokenize(text.lower())\n",
        "        unique_words = set(words)\n",
        "        unique_word_count = len(unique_words)\n",
        "\n",
        "        if len(words) > 0:\n",
        "            info_density = unique_word_count / len(words)\n",
        "        else:\n",
        "            info_density = 0\n",
        "\n",
        "        relevance_score = sentence_count * info_density\n",
        "\n",
        "        return relevance_score\n",
        "\n",
        "    def _extract_text_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
        "        page_texts = []\n",
        "        try:\n",
        "            with open(pdf_path, \"rb\") as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                start_page = 0\n",
        "                if len(reader.pages) > 1:\n",
        "                    first_page_text = reader.pages[0].extract_text() or \"\"\n",
        "                    if self._is_cover_page(first_page_text):\n",
        "                        start_page = 1\n",
        "                        print(\"Skipping cover page...\")\n",
        "\n",
        "                for page_num, page in enumerate(reader.pages[start_page:], start_page + 1):\n",
        "                    page_text = page.extract_text() or \"\"\n",
        "\n",
        "\n",
        "                    if not page_text.strip() or self._is_cover_page(page_text):\n",
        "                        continue\n",
        "\n",
        "                    cleaned_text = self._clean_noise(page_text)\n",
        "\n",
        "                    relevance_score = self._calculate_content_relevance(cleaned_text)\n",
        "\n",
        "                    if relevance_score > 5:\n",
        "                        page_texts.append({\n",
        "                            'text': cleaned_text,\n",
        "                            'page': page_num,\n",
        "                            'source': 'direct',\n",
        "                            'relevance': relevance_score\n",
        "                        })\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text directly: {str(e)}\")\n",
        "\n",
        "        return page_texts\n",
        "\n",
        "    def _extract_text_from_pdf_images(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
        "        page_texts = []\n",
        "        try:\n",
        "            images = convert_from_path(pdf_path)\n",
        "\n",
        "            start_page = 0\n",
        "            if len(images) > 1:\n",
        "                first_page_text = pytesseract.image_to_string(images[0])\n",
        "                if self._is_cover_page(first_page_text):\n",
        "                    start_page = 1\n",
        "                    print(\"Skipping cover page image...\")\n",
        "\n",
        "            for i, image in enumerate(images[start_page:], start_page + 1):\n",
        "                page_text = pytesseract.image_to_string(image)\n",
        "\n",
        "                if not page_text.strip() or self._is_cover_page(page_text):\n",
        "                    continue\n",
        "\n",
        "                cleaned_text = self._clean_noise(page_text)\n",
        "\n",
        "                relevance_score = self._calculate_content_relevance(cleaned_text)\n",
        "\n",
        "                if relevance_score > 5:\n",
        "                    page_texts.append({\n",
        "                        'text': cleaned_text,\n",
        "                        'page': i,\n",
        "                        'source': 'image',\n",
        "                        'relevance': relevance_score\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from images: {str(e)}\")\n",
        "\n",
        "        return page_texts\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s.,;:!?()-]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _extract_important_phrases(self, page_texts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        important_phrases = []\n",
        "\n",
        "        for page_data in page_texts:\n",
        "            text = page_data['text']\n",
        "            page_num = page_data['page']\n",
        "\n",
        "            sentences = sent_tokenize(text)\n",
        "            for sentence in sentences:\n",
        "                words = word_tokenize(sentence)\n",
        "                tagged_words = pos_tag(words)\n",
        "\n",
        "                current_phrase = []\n",
        "                for word, tag in tagged_words:\n",
        "                    if tag in self.important_pos_tags and word.lower() not in stopwords.words('english'):\n",
        "                        current_phrase.append(word)\n",
        "                    elif current_phrase:\n",
        "                        if len(current_phrase) >= 1:\n",
        "                            phrase = ' '.join(current_phrase)\n",
        "                            important_phrases.append({\n",
        "                                'phrase': phrase,\n",
        "                                'page': page_num,\n",
        "                                'context': sentence\n",
        "                            })\n",
        "                        current_phrase = []\n",
        "\n",
        "                if current_phrase and len(current_phrase) >= 1:\n",
        "                    phrase = ' '.join(current_phrase)\n",
        "                    important_phrases.append({\n",
        "                        'phrase': phrase,\n",
        "                        'page': page_num,\n",
        "                        'context': sentence\n",
        "                    })\n",
        "\n",
        "        unique_phrases = {}\n",
        "        for item in important_phrases:\n",
        "            phrase = item['phrase']\n",
        "            if phrase not in unique_phrases or len(phrase) > len(unique_phrases[phrase]['phrase']):\n",
        "                unique_phrases[phrase] = item\n",
        "\n",
        "        result = list(unique_phrases.values())\n",
        "        result.sort(key=lambda x: len(x['phrase']), reverse=True)\n",
        "\n",
        "        return result[:50]\n",
        "\n",
        "    def _extract_key_sentences(self, page_texts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        key_sentences = []\n",
        "\n",
        "        importance_indicators = [\n",
        "            'important', 'significant', 'key', 'main', 'primary', 'essential', 'critical',\n",
        "            'fundamental', 'crucial', 'vital', 'major', 'central', 'core', 'notable',\n",
        "            'define', 'definition', 'means', 'refer', 'example', 'instance', 'illustrate',\n",
        "            'demonstrate', 'show', 'prove', 'evidence', 'research', 'study', 'analysis',\n",
        "            'conclude', 'summary', 'therefore', 'thus', 'hence', 'consequently'\n",
        "        ]\n",
        "\n",
        "        for page_data in page_texts:\n",
        "            text = page_data['text']\n",
        "            page_num = page_data['page']\n",
        "\n",
        "            sentences = sent_tokenize(text)\n",
        "            for sentence in sentences:\n",
        "                if any(indicator in sentence.lower() for indicator in importance_indicators):\n",
        "                    key_sentences.append({\n",
        "                        'sentence': sentence,\n",
        "                        'page': page_num\n",
        "                    })\n",
        "\n",
        "                elif re.search(r'\\b[A-Z][a-z]+ (is|are|refers to|means|can be defined as)\\b', sentence):\n",
        "                    key_sentences.append({\n",
        "                        'sentence': sentence,\n",
        "                        'page': page_num\n",
        "                    })\n",
        "\n",
        "        return key_sentences\n",
        "\n",
        "    def _find_answer_for_phrase(self, phrase: str, page_texts: List[Dict[str, Any]], context: str = None) -> Dict[str, Any]:\n",
        "        for page_data in page_texts:\n",
        "            text = page_data['text']\n",
        "            page_num = page_data['page']\n",
        "\n",
        "            definition_patterns = [\n",
        "                f\"{phrase} is \",\n",
        "                f\"{phrase} are \",\n",
        "                f\"{phrase} refers to \",\n",
        "                f\"{phrase} means \",\n",
        "                f\"{phrase} can be defined as \",\n",
        "                f\"define {phrase} as \",\n",
        "                f\"{phrase} is defined as \"\n",
        "            ]\n",
        "\n",
        "            sentences = sent_tokenize(text)\n",
        "            for sentence in sentences:\n",
        "                if any(pattern.lower() in sentence.lower() for pattern in definition_patterns):\n",
        "                    return {\n",
        "                        'answer': sentence,\n",
        "                        'page': page_num\n",
        "                    }\n",
        "\n",
        "        if context:\n",
        "            for page_data in page_texts:\n",
        "                if context in page_data['text']:\n",
        "                    return {\n",
        "                        'answer': context,\n",
        "                        'page': page_data['page']\n",
        "                    }\n",
        "\n",
        "        relevant_sentences = []\n",
        "        for page_data in page_texts:\n",
        "            text = page_data['text']\n",
        "            page_num = page_data['page']\n",
        "\n",
        "            sentences = sent_tokenize(text)\n",
        "            for sentence in sentences:\n",
        "                if phrase.lower() in sentence.lower():\n",
        "                    relevant_sentences.append({\n",
        "                        'sentence': sentence,\n",
        "                        'page': page_num,\n",
        "                        'relevance': len(sentence)\n",
        "                    })\n",
        "\n",
        "        relevant_sentences.sort(key=lambda x: x['relevance'])\n",
        "\n",
        "        if relevant_sentences:\n",
        "            best_match = relevant_sentences[0]\n",
        "            return {\n",
        "                'answer': best_match['sentence'],\n",
        "                'page': best_match['page']\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'answer': f\"Information about {phrase} can be found in the document.\",\n",
        "            'page': 'unknown'\n",
        "        }\n",
        "\n",
        "    def _generate_factual_questions_with_answers(self, phrases: List[Dict[str, Any]], page_texts: List[Dict[str, Any]], num: int) -> List[Dict[str, Any]]:\n",
        "        questions = []\n",
        "        templates = self.question_templates['factual']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for phrase_data in phrases_copy[:num]:\n",
        "            phrase = phrase_data['phrase']\n",
        "            context = phrase_data.get('context')\n",
        "            template = random.choice(templates)\n",
        "            question = template.format(phrase)\n",
        "\n",
        "            answer_data = self._find_answer_for_phrase(phrase, page_texts, context)\n",
        "\n",
        "            questions.append({\n",
        "                'question': question,\n",
        "                'answer': answer_data['answer'],\n",
        "                'page': answer_data['page'],\n",
        "                'type': 'factual'\n",
        "            })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_conceptual_questions_with_answers(self, phrases: List[Dict[str, Any]], page_texts: List[Dict[str, Any]], num: int) -> List[Dict[str, Any]]:\n",
        "        questions = []\n",
        "        templates = self.question_templates['conceptual']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for i, template in enumerate(random.sample(templates, min(num, len(templates)))):\n",
        "            if \"{}\" in template and \"{}\" in template[template.index(\"{}\") + 2:]:\n",
        "                if len(phrases_copy) >= 2:\n",
        "                    phrase_data1 = phrases_copy.pop(0)\n",
        "                    phrase_data2 = phrases_copy.pop(0)\n",
        "                    phrase1 = phrase_data1['phrase']\n",
        "                    phrase2 = phrase_data2['phrase']\n",
        "                    question = template.format(phrase1, phrase2)\n",
        "\n",
        "                    both_phrases = f\"{phrase1} and {phrase2}\"\n",
        "                    answer_data = self._find_answer_for_phrase(both_phrases, page_texts)\n",
        "\n",
        "                    if answer_data['page'] == 'unknown':\n",
        "                        answer_data = self._find_answer_for_phrase(phrase1, page_texts, phrase_data1.get('context'))\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': answer_data['answer'],\n",
        "                        'page': answer_data['page'],\n",
        "                        'type': 'conceptual'\n",
        "                    })\n",
        "            else:\n",
        "                if phrases_copy:\n",
        "                    phrase_data = phrases_copy.pop(0)\n",
        "                    phrase = phrase_data['phrase']\n",
        "                    question = template.format(phrase)\n",
        "\n",
        "                    answer_data = self._find_answer_for_phrase(phrase, page_texts, phrase_data.get('context'))\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': answer_data['answer'],\n",
        "                        'page': answer_data['page'],\n",
        "                        'type': 'conceptual'\n",
        "                    })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_analytical_questions_with_answers(self, phrases: List[Dict[str, Any]], key_sentences: List[Dict[str, Any]], page_texts: List[Dict[str, Any]], num: int) -> List[Dict[str, Any]]:\n",
        "        questions = []\n",
        "        templates = self.question_templates['analytical']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for i, template in enumerate(random.sample(templates, min(num, len(templates)))):\n",
        "            if \"{}\" in template and \"{}\" in template[template.index(\"{}\") + 2:]:\n",
        "                if len(phrases_copy) >= 2:\n",
        "                    phrase_data1 = phrases_copy.pop(0)\n",
        "                    phrase_data2 = phrases_copy.pop(0)\n",
        "                    phrase1 = phrase_data1['phrase']\n",
        "                    phrase2 = phrase_data2['phrase']\n",
        "                    question = template.format(phrase1, phrase2)\n",
        "\n",
        "                    answer1 = self._find_answer_for_phrase(phrase1, page_texts, phrase_data1.get('context'))\n",
        "                    answer2 = self._find_answer_for_phrase(phrase2, page_texts, phrase_data2.get('context'))\n",
        "\n",
        "                    combined_answer = f\"Regarding {phrase1}: {answer1['answer']} Regarding {phrase2}: {answer2['answer']}\"\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': combined_answer,\n",
        "                        'page': f\"{answer1['page']} and {answer2['page']}\",\n",
        "                        'type': 'analytical'\n",
        "                    })\n",
        "            else:\n",
        "                if phrases_copy:\n",
        "                    phrase_data = phrases_copy.pop(0)\n",
        "                    phrase = phrase_data['phrase']\n",
        "                    question = template.format(phrase)\n",
        "\n",
        "                    answer_data = self._find_answer_for_phrase(phrase, page_texts, phrase_data.get('context'))\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': answer_data['answer'],\n",
        "                        'page': answer_data['page'],\n",
        "                        'type': 'analytical'\n",
        "                    })\n",
        "\n",
        "        if key_sentences and len(questions) < num:\n",
        "            for sentence_data in random.sample(key_sentences, min(num - len(questions), len(key_sentences))):\n",
        "                sentence = sentence_data['sentence']\n",
        "                page = sentence_data['page']\n",
        "\n",
        "                question = f\"Analyze the following statement: '{sentence}'\"\n",
        "\n",
        "                context_answer = self._find_context_for_sentence(sentence, page_texts, page)\n",
        "\n",
        "                questions.append({\n",
        "                    'question': question,\n",
        "                    'answer': context_answer['answer'],\n",
        "                    'page': context_answer['page'],\n",
        "                    'type': 'analytical'\n",
        "                })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _find_context_for_sentence(self, sentence: str, page_texts: List[Dict[str, Any]], target_page: int) -> Dict[str, Any]:\n",
        "        for page_data in page_texts:\n",
        "            if page_data['page'] == target_page:\n",
        "                text = page_data['text']\n",
        "\n",
        "                sentences = sent_tokenize(text)\n",
        "                for i, s in enumerate(sentences):\n",
        "                    if sentence in s:\n",
        "                        context = []\n",
        "                        if i > 0:\n",
        "                            context.append(sentences[i-1])\n",
        "                        context.append(s)\n",
        "                        if i < len(sentences) - 1:\n",
        "                            context.append(sentences[i+1])\n",
        "\n",
        "                        return {\n",
        "                            'answer': ' '.join(context),\n",
        "                            'page': target_page\n",
        "                        }\n",
        "\n",
        "        return {\n",
        "            'answer': sentence,\n",
        "            'page': target_page\n",
        "        }\n",
        "\n",
        "    def _generate_application_questions_with_answers(self, phrases: List[Dict[str, Any]], page_texts: List[Dict[str, Any]], num: int) -> List[Dict[str, Any]]:\n",
        "        questions = []\n",
        "        templates = self.question_templates['application']\n",
        "\n",
        "        phrases_copy = phrases.copy()\n",
        "        random.shuffle(phrases_copy)\n",
        "\n",
        "        for i, template in enumerate(random.sample(templates, min(num, len(templates)))):\n",
        "            if \"{}\" in template and \"{}\" in template[template.index(\"{}\") + 2:]:\n",
        "                if len(phrases_copy) >= 2:\n",
        "                    phrase_data1 = phrases_copy.pop(0)\n",
        "                    phrase_data2 = phrases_copy.pop(0)\n",
        "                    phrase1 = phrase_data1['phrase']\n",
        "                    phrase2 = phrase_data2['phrase']\n",
        "                    question = template.format(phrase1, phrase2)\n",
        "\n",
        "                    answer1 = self._find_answer_for_phrase(phrase1, page_texts, phrase_data1.get('context'))\n",
        "                    answer2 = self._find_answer_for_phrase(phrase2, page_texts, phrase_data2.get('context'))\n",
        "\n",
        "                    combined_answer = f\"Based on the document: Regarding {phrase1}: {answer1['answer']} Regarding {phrase2}: {answer2['answer']}\"\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': combined_answer,\n",
        "                        'page': f\"{answer1['page']} and {answer2['page']}\",\n",
        "                        'type': 'application'\n",
        "                    })\n",
        "            else:\n",
        "                if phrases_copy:\n",
        "                    phrase_data = phrases_copy.pop(0)\n",
        "                    phrase = phrase_data['phrase']\n",
        "                    question = template.format(phrase)\n",
        "\n",
        "                    answer_data = self._find_answer_for_phrase(phrase, page_texts, phrase_data.get('context'))\n",
        "\n",
        "                    answer = f\"Based on the document: {answer_data['answer']}\"\n",
        "\n",
        "                    questions.append({\n",
        "                        'question': question,\n",
        "                        'answer': answer,\n",
        "                        'page': answer_data['page'],\n",
        "                        'type': 'application'\n",
        "                    })\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def generate_questions_with_answers_from_pdf(self, pdf_path: str, num_questions: int = 10) -> List[Dict[str, Any]]:\n",
        "        print(\"Extracting text from PDF and filtering out less necessary content...\")\n",
        "\n",
        "        direct_page_texts = self._extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        image_page_texts = self._extract_text_from_pdf_images(pdf_path)\n",
        "\n",
        "        all_page_texts = direct_page_texts + image_page_texts\n",
        "\n",
        "        all_page_texts.sort(key=lambda x: x.get('relevance', 0), reverse=True)\n",
        "\n",
        "        most_relevant_pages = all_page_texts[:min(len(all_page_texts), 20)]\n",
        "\n",
        "        for page_data in most_relevant_pages:\n",
        "            page_data['text'] = self._clean_text(page_data['text'])\n",
        "\n",
        "        total_text = ''.join(page_data['text'] for page_data in most_relevant_pages)\n",
        "        if not total_text or len(total_text) < 50:\n",
        "            return [{\n",
        "                'question': \"The PDF doesn't contain enough extractable text to generate questions.\",\n",
        "                'answer': \"Please try a different PDF with more text content.\",\n",
        "                'page': \"N/A\",\n",
        "                'type': 'error'\n",
        "            }]\n",
        "\n",
        "        print(f\"Successfully extracted content from {len(most_relevant_pages)} relevant pages.\")\n",
        "\n",
        "        important_phrases = self._extract_important_phrases(most_relevant_pages)\n",
        "        key_sentences = self._extract_key_sentences(most_relevant_pages)\n",
        "\n",
        "        print(f\"Identified {len(important_phrases)} important phrases and {len(key_sentences)} key sentences.\")\n",
        "\n",
        "        num_factual = max(1, int(num_questions * 0.4))\n",
        "        num_conceptual = max(1, int(num_questions * 0.3))\n",
        "        num_analytical = max(1, int(num_questions * 0.2))\n",
        "        num_application = max(1, num_questions - num_factual - num_conceptual - num_analytical)\n",
        "\n",
        "        print(\"Generating questions and finding answers...\")\n",
        "\n",
        "        factual_questions = self._generate_factual_questions_with_answers(important_phrases, most_relevant_pages, num_factual)\n",
        "        conceptual_questions = self._generate_conceptual_questions_with_answers(important_phrases, most_relevant_pages, num_conceptual)\n",
        "        analytical_questions = self._generate_analytical_questions_with_answers(important_phrases, key_sentences, most_relevant_pages, num_analytical)\n",
        "        application_questions = self._generate_application_questions_with_answers(important_phrases, most_relevant_pages, num_application)\n",
        "\n",
        "        all_questions = factual_questions + conceptual_questions + analytical_questions + application_questions\n",
        "\n",
        "        if len(all_questions) < num_questions and important_phrases:\n",
        "            additional_needed = num_questions - len(all_questions)\n",
        "            additional_questions = self._generate_factual_questions_with_answers(important_phrases, most_relevant_pages, additional_needed)\n",
        "            all_questions.extend(additional_questions)\n",
        "\n",
        "        random.shuffle(all_questions)\n",
        "\n",
        "        print(f\"Successfully generated {len(all_questions)} questions with answers.\")\n",
        "\n",
        "        return all_questions[:num_questions]\n",
        "\n",
        "def generate_questions_with_answers_from_uploaded_pdf(num_questions=10):\n",
        "    print(\"Please upload a PDF file...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file was uploaded.\")\n",
        "        return\n",
        "\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "\n",
        "    print(f\"\\nProcessing {file_name}...\")\n",
        "    print(\"This may take a few minutes depending on the PDF size and complexity.\")\n",
        "\n",
        "    generator = QGenerator()\n",
        "\n",
        "    questions_with_answers = generator.generate_questions_with_answers_from_pdf(file_name, num_questions)\n",
        "\n",
        "    print(f\"\\nGenerated {len(questions_with_answers)} questions with answers from {file_name}:\")\n",
        "    for i, qa in enumerate(questions_with_answers, 1):\n",
        "        print(f\"\\n{i}. {qa['question']}\")\n",
        "        print(f\"   Answer: {qa['answer']}\")\n",
        "        print(f\"   Page: {qa['page']}\")\n",
        "        print(f\"   Type: {qa['type']}\")\n",
        "\n",
        "    return questions_with_answers"
      ],
      "metadata": {
        "id": "eMiBPVBkcoBd"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_questions = 30\n",
        "\n",
        "questions_with_answers = generate_questions_with_answers_from_uploaded_pdf(num_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vBn32AQanX1I",
        "outputId": "3395a7fe-63fe-4e5e-83b0-cd65228aa8ce"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload a PDF file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aab64362-3733-4ee3-a8bd-c05e4db2ce82\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aab64362-3733-4ee3-a8bd-c05e4db2ce82\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Research .pdf to Research  (5).pdf\n",
            "\n",
            "Processing Research  (5).pdf...\n",
            "This may take a few minutes depending on the PDF size and complexity.\n",
            "Extracting text from PDF and filtering out less necessary content...\n",
            "Skipping cover page...\n",
            "Skipping cover page image...\n",
            "Successfully extracted content from 4 relevant pages.\n",
            "Identified 50 important phrases and 22 key sentences.\n",
            "Generating questions and finding answers...\n",
            "Successfully generated 30 questions with answers.\n",
            "\n",
            "Generated 30 questions with answers from Research  (5).pdf:\n",
            "\n",
            "1. How can Understanding eigenvalues be applied to solve fundamental concepts?\n",
            "   Answer: Based on the document: Regarding Understanding eigenvalues: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing. Regarding fundamental concepts: Introduction Eigenvalues and eigenvectors are fundamental concepts in linear algebra, which play a significant role in various fields of science and engineering.\n",
            "   Page: 2 and 2\n",
            "   Type: application\n",
            "\n",
            "2. Compare and contrast facial recognition and machine learning models.\n",
            "   Answer: Regarding facial recognition: Computer Vision and Image Compression Use Case: Image compression using PCA or Eigenfaces in facial recognition. Regarding machine learning models: Benefit: Simplifies data, reduces noise, and speeds up machine learning models.\n",
            "   Page: 3 and 3\n",
            "   Type: analytical\n",
            "\n",
            "3. Who is dominant eigenvector?\n",
            "   Answer: It calculates the dominant eigenvector of a modified link matrix to determine the importance of each webpage.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "4. How does machine learning models relate to computer graphics?\n",
            "   Answer: Benefit: Simplifies data, reduces noise, and speeds up machine learning models.\n",
            "   Page: 3\n",
            "   Type: conceptual\n",
            "\n",
            "5. What is the significance of significant role?\n",
            "   Answer: Introduction Eigenvalues and eigenvectors are fundamental concepts in linear algebra, which play a significant role in various fields of science and engineering.\n",
            "   Page: 2\n",
            "   Type: conceptual\n",
            "\n",
            "6. What would happen if uses eigenvector centrality changed?\n",
            "   Answer: How: Googles PageRank treats the web as a graph and uses eigenvector centrality to rank pages.\n",
            "   Page: 3\n",
            "   Type: analytical\n",
            "\n",
            "7. What are the strengths and weaknesses of Understanding eigenvalues?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: analytical\n",
            "\n",
            "8. Who is Computer Science Eigenvalues?\n",
            "   Answer: Applications in Computer Science Eigenvalues and eigenvectors have a wide range of applications in computer science.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "9. What are the characteristics of eigenface recognition?\n",
            "   Answer: How: In eigenface recognition, each face image is projected onto the eigenvectors (eigenfaces) of the training set.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "10. How would you explain magnitude changes to someone new to the subject?\n",
            "   Answer: The directions that remain the same (even if the magnitude changes) under the transformation are represented by eigenvectors.\n",
            "   Page: 2\n",
            "   Type: conceptual\n",
            "\n",
            "11. What is finding influential nodes?\n",
            "   Answer: Graph Theory and Network Analysis Use Case: Analyzing social networks, finding influential nodes.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "12. What is vectors represent key facial features?\n",
            "   Answer: These vectors represent key facial features.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "13. Where is machine learning models located?\n",
            "   Answer: Benefit: Simplifies data, reduces noise, and speeds up machine learning models.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "14. Who is computer graphics?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "15. What evidence supports Network Analysis Use Case?\n",
            "   Answer: Graph Theory and Network Analysis Use Case: Analyzing social networks, finding influential nodes.\n",
            "   Page: 3\n",
            "   Type: analytical\n",
            "\n",
            "16. How would you implement Googles PageRank Algorithm Use Case in practice?\n",
            "   Answer: Based on the document: Googles PageRank Algorithm Use Case: Ranking web pages in search engines.\n",
            "   Page: 3\n",
            "   Type: application\n",
            "\n",
            "17. What are the characteristics of principal components?\n",
            "   Answer: How: PCA uses the eigenvectors of the datas covariance matrix to identify the directions (principal components) where the data varies the most.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "18. What is the definition of Amatrix represents?\n",
            "   Answer: Understanding the Concept Lets break it down with a geometric interpretation:  Amatrix represents a linear transformation (such as rotation, scaling, or shearing).\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "19. What is the purpose of Dimensionality Reduction?\n",
            "   Answer: Principal Component Analysis (PCA) Use Case: Dimensionality Reduction, Feature Extraction in Machine Learning.\n",
            "   Page: 3\n",
            "   Type: conceptual\n",
            "\n",
            "20. Where is modified link matrix located?\n",
            "   Answer: It calculates the dominant eigenvector of a modified link matrix to determine the importance of each webpage.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "21. How would you evaluate the effectiveness of magnitude changes?\n",
            "   Answer: The directions that remain the same (even if the magnitude changes) under the transformation are represented by eigenvectors.\n",
            "   Page: 2\n",
            "   Type: analytical\n",
            "\n",
            "22. What are the characteristics of corresponding eigenvalues tell?\n",
            "   Answer: The corresponding eigenvalues tell us the importance (variance) of each direction.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "23. What are the implications of Principal Component Analysis?\n",
            "   Answer: Principal Component Analysis (PCA) Use Case: Dimensionality Reduction, Feature Extraction in Machine Learning.\n",
            "   Page: 3\n",
            "   Type: conceptual\n",
            "\n",
            "24. What conclusions can be drawn from computer graphics?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: analytical\n",
            "\n",
            "25. What is the definition of machine learning?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "26. How does Network Analysis Use Case affect Machine Learning?\n",
            "   Answer: Graph Theory and Network Analysis Use Case: Analyzing social networks, finding influential nodes.\n",
            "   Page: 3\n",
            "   Type: conceptual\n",
            "\n",
            "27. How could image processing be used to improve computer graphics?\n",
            "   Answer: Based on the document: Regarding image processing: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing. Regarding computer graphics: Mathematically, for a square matrix A, a vector v is an eigenvector and A is A-v 2 eV the eigenvalue if: Understanding eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2 and 2\n",
            "   Type: application\n",
            "\n",
            "28. What are the characteristics of Computer Vision?\n",
            "   Answer: Computer Vision and Image Compression Use Case: Image compression using PCA or Eigenfaces in facial recognition.\n",
            "   Page: 3\n",
            "   Type: factual\n",
            "\n",
            "29. Who is Understandin g eigenvalues?\n",
            "   Answer: Mathematically, for a square matrix A, a vector v is an eigenvector and λ is the eigenvalue if: Understandin g eigenvalues and eigenvectors allows us to grasp the behavior of linear transformations and systems, making them essential in fields like computer graphics, machine learning, data science, and image processing.\n",
            "   Page: 2\n",
            "   Type: factual\n",
            "\n",
            "30. Why is eigenface recognition important?\n",
            "   Answer: How: In eigenface recognition, each face image is projected onto the eigenvectors (eigenfaces) of the training set.\n",
            "   Page: 3\n",
            "   Type: conceptual\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if questions_with_answers:\n",
        "    with open('questions_and_answers.txt', 'w') as f:\n",
        "        for i, qa in enumerate(questions_with_answers, 1):\n",
        "            f.write(f\"{i}. {qa['question']}\\n\")\n",
        "            f.write(f\"   Answer: {qa['answer']}\\n\")\n",
        "            f.write(f\"   Page: {qa['page']}\\n\")\n",
        "            f.write(f\"   Type: {qa['type']}\\n\\n\")\n",
        "\n",
        "    files.download('questions_and_answers.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "B9HRXaDGnaKO",
        "outputId": "3e37d6e7-6520-44f1-e782-1a8a59a09f6d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e86b50cd-22fc-40b4-b089-3a50dc7d6087\", \"questions_and_answers.txt\", 8562)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}